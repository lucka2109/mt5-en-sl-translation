{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP+GKiB8NCjPCUaMQzh273s"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqpMPYQNViBz"
      },
      "outputs": [],
      "source": [
        "# Install essential libraries for NLP tasks\n",
        "!pip install transformers sentencepiece datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for data loading, visualization, deep learning, and\n",
        "# NLP model handling\n",
        "from datasets import load_dataset\n",
        "from google.colab import drive\n",
        "from IPython.display import display\n",
        "from IPython.html import widgets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "from transformers import AdamW, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "sns.set()"
      ],
      "metadata": {
        "id": "fsLs02isYN8m",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model repository\n",
        "model_repo = 'google/mt5-small'\n",
        "max_seq_len = model.config.max_length"
      ],
      "metadata": {
        "id": "7FQ_7dbfXTh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer from the pre-trained model repository\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_repo)"
      ],
      "metadata": {
        "id": "e1o3FhacYEqq",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained sequence-to-sequence model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_repo)\n",
        "# Move the model to the GPU for faster computation\n",
        "model = model.cuda()"
      ],
      "metadata": {
        "id": "mBSAtL6GZk18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sent = 'Here is our test sentence!'\n",
        "# Encode the input sentence into token IDs and move to GPU\n",
        "token_ids = tokenizer.encode(input_sent, return_tensors='pt').cuda()\n",
        "token_ids\n",
        "\n",
        "# Generate model output:\n",
        "model_out = model.generate(token_ids)\n",
        "print(model_out)\n",
        "\n",
        "# Convert token IDs back to text\n",
        "output_text = tokenizer.convert_tokens_to_string(\n",
        "    tokenizer.convert_ids_to_tokens(model_out[0]))\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "2P9x0VSwbZom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode an example input string into token IDs, then convert the token IDs back\n",
        "# to tokens for verification\n",
        "example_input_str = '<sl>This is a test nbuig.'\n",
        "input_ids = tokenizer.encode(example_input_str, return_tensors='pt')\n",
        "print('Input IDs: ', input_ids)\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "print('Tokens: ', tokens)"
      ],
      "metadata": {
        "id": "11cJf2TtdNYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort and display the tokenizer's vocabulary based on token IDs\n",
        "sorted(tokenizer.vocab.items(), key=lambda x: x[1])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "m7dkdqgXitXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the English to Slovenian subset of the opus-100 dataset from the\n",
        "# Helsinki-NLP collection\n",
        "dataset = load_dataset('Helsinki-NLP/opus-100', 'en-sl')"
      ],
      "metadata": {
        "id": "fZvKfn7AkZeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the loaded dataset into training and testing datasets\n",
        "train_dataset = dataset['train']\n",
        "test_dataset = dataset['test']"
      ],
      "metadata": {
        "id": "PGOKp4d_mczJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the training dataset to inspect its contents\n",
        "train_dataset"
      ],
      "metadata": {
        "id": "kznis4Z0mwz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the testing dataset to inspect its contents\n",
        "test_dataset"
      ],
      "metadata": {
        "id": "I56r9l26m4iW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first example from the training dataset to inspect a single data\n",
        "# entry\n",
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "cjlrzysdnDK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary that maps language codes to special tokens representing\n",
        "# each language\n",
        "LANG_TOKEN_MAPPING = {\n",
        "    'en': '<en>',\n",
        "    'sl': '<sl>'\n",
        "}"
      ],
      "metadata": {
        "id": "z81vRE6B9tlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add special tokens for language indicators to the tokenizer and update the\n",
        "# model's token embeddings\n",
        "special_tokens_dict = {'additional_special_tokens': list(LANG_TOKEN_MAPPING.values())}\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "DnMDe0JLoLZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the example input string into token IDs with padding and truncation,\n",
        "# and return as a PyTorch tensor\n",
        "token_ids = tokenizer.encode(\n",
        "    example_input_str, return_tensors='pt',\n",
        "    padding='max_length',\n",
        "    truncation=True, max_length=max_seq_len)\n",
        "print(token_ids)"
      ],
      "metadata": {
        "id": "PTtLC_qTpzzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to encode input text for model input, including special language tokens,\n",
        "# with padding and truncation to ensure consistent sequence length\n",
        "def encode_input_str(text, target_lang, tokenizer, seq_len,\n",
        "                     lang_token_map=LANG_TOKEN_MAPPING):\n",
        "  target_lang_token = lang_token_map[target_lang]\n",
        "\n",
        "  # Tokenize and add special tokens\n",
        "  input_ids = tokenizer.encode(\n",
        "      text = target_lang_token + text,\n",
        "      return_tensors = 'pt',\n",
        "      padding = 'max_length',\n",
        "      truncation = True,\n",
        "      max_length = seq_len)\n",
        "\n",
        "  return input_ids[0]\n",
        "\n",
        "# Function to encode target text into token IDs with padding and truncation,\n",
        "# ensuring uniform sequence length for model input\n",
        "def encode_target_str(text, tokenizer, seq_len,\n",
        "                      lang_token_map=LANG_TOKEN_MAPPING):\n",
        "  token_ids = tokenizer.encode(\n",
        "      text = text,\n",
        "      return_tensors = 'pt',\n",
        "      padding = 'max_length',\n",
        "      truncation = True,\n",
        "      max_length = seq_len)\n",
        "\n",
        "  return token_ids[0]\n",
        "\n",
        "# Function to format translation data by selecting random language pairs,\n",
        "# encoding their texts into token IDs, and returning the encoded input and\n",
        "# target sequences\n",
        "def format_translation_data(translations, lang_token_map,\n",
        "                              tokenizer, seq_len=128):\n",
        "  langs = list(lang_token_map.keys())\n",
        "  input_lang, target_lang = np.random.choice(langs, size=2, replace=False)\n",
        "\n",
        "  # Get the translations for the batch\n",
        "  input_text = translations[input_lang]\n",
        "  target_text = translations[target_lang]\n",
        "\n",
        "  if input_text is None or target_text is None:\n",
        "      return None\n",
        "\n",
        "  input_token_ids = encode_input_str(\n",
        "      input_text, target_lang, tokenizer, seq_len, lang_token_map)\n",
        "\n",
        "  target_token_ids = encode_target_str(\n",
        "      target_text, tokenizer, seq_len, lang_token_map)\n",
        "\n",
        "  return input_token_ids, target_token_ids\n",
        "\n",
        "# Process a batch of translation data by formatting and encoding each translation set,\n",
        "# concatenate the encoded inputs and targets into tensors, and move them to the\n",
        "# GPU\n",
        "def transform_batch(batch, lang_token_map, tokenizer):\n",
        "  inputs = []\n",
        "  targets = []\n",
        "  for translation_set in batch['translation']:\n",
        "    formatted_data = format_translation_data(\n",
        "        translation_set, lang_token_map, tokenizer, max_seq_len)\n",
        "\n",
        "    if formatted_data is None:\n",
        "      continue\n",
        "\n",
        "    input_ids, target_ids = formatted_data\n",
        "    inputs.append(input_ids.unsqueeze(0))\n",
        "    targets.append(target_ids.unsqueeze(0))\n",
        "\n",
        "  batch_input_ids = torch.cat(inputs).cuda()\n",
        "  batch_target_ids = torch.cat(targets).cuda()\n",
        "\n",
        "  return batch_input_ids, batch_target_ids\n",
        "\n",
        "# Generate batches of formatted and encoded data from the shuffled dataset, with\n",
        "# each batch processed by the 'transform_batch' function for use in model\n",
        "# training or evaluation\n",
        "def get_data_generator(dataset, lang_token_map, tokenizer, batch_size=32):\n",
        "  dataset = dataset.shuffle()\n",
        "  for i in range(0, len(dataset), batch_size):\n",
        "    raw_batch = dataset[i:i+batch_size]\n",
        "    yield transform_batch(raw_batch, lang_token_map, tokenizer)"
      ],
      "metadata": {
        "id": "h8ZMWyLrqbft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_ids, out_ids = format_translation_data(\n",
        "    train_dataset[1]['translation'], LANG_TOKEN_MAPPING, tokenizer)\n",
        "\n",
        "print(' '.join(tokenizer.convert_ids_to_tokens(in_ids)))\n",
        "print(' '.join(tokenizer.convert_ids_to_tokens(out_ids)))\n",
        "\n",
        "data_gen = get_data_generator(train_dataset, LANG_TOKEN_MAPPING, tokenizer, 8)\n",
        "data_batch = next(data_gen)\n",
        "\n",
        "print('Input shape:', data_batch[0].shape)\n",
        "print('Output shape:', data_batch[1].shape)"
      ],
      "metadata": {
        "id": "zQK8xiVzc4J-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 5\n",
        "batch_size = 16\n",
        "print_freq = 50\n",
        "lr = 5e-4\n",
        "n_batches = int(np.ceil(len(train_dataset) / batch_size))\n",
        "total_steps = n_epochs * n_batches\n",
        "n_warmup_steps = int(total_steps * 0.01)"
      ],
      "metadata": {
        "id": "evIryY4Cdyww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=lr)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, n_warmup_steps, total_steps)"
      ],
      "metadata": {
        "id": "n9-fbDhlkdnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []"
      ],
      "metadata": {
        "id": "mX0cnIialJpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, gdataset, max_iters=8):\n",
        "  test_generator = get_data_generator(gdataset, LANG_TOKEN_MAPPING,\n",
        "                                      tokenizer, batch_size)\n",
        "  eval_losses = []\n",
        "  for i, (input_batch, label_batch) in enumerate(test_generator):\n",
        "    if i >= max_iters:\n",
        "      break\n",
        "\n",
        "    model_out = model.forward(\n",
        "        input_ids = input_batch,\n",
        "        labels = label_batch)\n",
        "    eval_losses.append(model_out.loss.item())\n",
        "\n",
        "  return np.mean(eval_losses)"
      ],
      "metadata": {
        "id": "73EtU_BNn1MM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = eval_model(model, test_dataset)"
      ],
      "metadata": {
        "id": "QBGWB-6jo3M8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss"
      ],
      "metadata": {
        "id": "wGPYFMj8pDHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch_idx in range(n_epochs):\n",
        "\n",
        "  # Randomize data order\n",
        "  data_generator = get_data_generator(train_dataset, LANG_TOKEN_MAPPING,\n",
        "                                      tokenizer, batch_size)\n",
        "\n",
        "  for batch_idx, (input_batch, label_batch) \\\n",
        "      in tqdm_notebook(enumerate(data_generator), total=n_batches):\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Forward pass\n",
        "      model_out = model.forward(\n",
        "          input_ids = input_batch,\n",
        "          labels = label_batch)\n",
        "\n",
        "      loss = model_out.loss\n",
        "      losses.append(loss.item())\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "\n",
        "      # Print training update info\n",
        "      if (batch_idx + 1) % print_freq == 0:\n",
        "        avg_loss = np.mean(losses[-print_freq:])\n",
        "        print('Epoch: {} | Step: {} | Avg. loss: {:.3f} | lr: {}'.format(\n",
        "            epoch_idx+1, batch_idx+1, avg_loss, scheduler.get_last_lr()[0]))\n",
        "\n",
        "  test_loss = eval_model(model, test_dataset)\n",
        "  print('Test loss of {:.3f}'.format(test_loss))"
      ],
      "metadata": {
        "id": "CS7W7Er-lQOu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}